---
layout: tutorial_slides
logo: "GTN"

title: "Scripting Galaxy using the API and BioBlend"
questions:
  - "What is a web API?"
  - "How to interact with Galaxy in a script?"
  - "Why should I use BioBlend?"
objectives:
  - "Get familiar with BioBlend using IPython"
  - "Launch a Galaxy job / Visualize your actions with Galaxy"
  - "Launch a Galaxy workflow / Visualize your actions with Galaxy"
requirements:
  -
    title: "IPython"
    type: "external"
    link: "https://ipython.org"
time_estimation: "2h"
key_points:
  - "Play with IPython"
  - "Understand the job launching in Galaxy with BioBlend"
  - "Understand the workflow launching in Galaxy with BioBlend"
  - "Be able to adapt this knowledge to personal workflows"
contributors:
  - odoppelt
  - fmareuil
  - nsoranzo
  - dannon
---

## Outline

* [Galaxy API](#api)
* [BioBlend](#bioblend)
* [Hands on basics](#handson)
* [Get familiar with BioBlend](#familiar)
* [Launch a Galaxy job](#launch)
* [Launch a Galaxy workflow](#workflow)

---

name: api
## Web API

- **Application Programming Interface (API)**: the protocol defined by a software for how it can be controlled by an external program
- A **web API** is an API for a web app which uses:
  - URLs for function definitions
  - HTTP requests as function calls
- Galaxy provides a rich API
  - The Galaxy UI is being migrated on top of the Galaxy API

???

The server providing the web API becomes the program, and the URLs become the commands - all through the medium of the HTTP protocol.

---

## Interacting with Galaxy: UI vs. API

- The Galaxy UI is good for:
  - exploring and visualizing data
  - experimenting
  - graphically designing workflows
  - people not comfortable with command line

- The Galaxy API is good for:
  - interact programmatically with the server
    - complex control: **branching** and **looping** (not possible in workflows)
  - automate repetitive tasks
  - integration with external resources

???

The web API allows you to use Galaxy's capabilities programmatically:
- run a workflow over each file in a directory
- run a second workflow only if the first passes some QA threshold
- upload a FASTQ file when the sequencer finishes writing it.

Additionally, the features that make Galaxy great are still applied when using the API:
- histories still capture the exact steps of your experiments and reproducibility is maintained
- jobs and compute resources are still managed for you
- datasets and metadata persist and are centralized
- your work is still sharable.

It's also worth noting that all the work you did via the API is still accessible and modifiable when you return to the UI.

---

## Galaxy API functionalities

- Users can:
  - manage histories and datasets
  - upload and download data
  - run tools and workflows, ...

- Admins can also manage:
  - data libraries
  - Tool Shed repositories
  - users, quotas, roles...

- Source code lives at https://github.com/galaxyproject/galaxy/tree/dev/lib/galaxy/webapps/galaxy/api/

???

Admin examples:
- upload files and start a workflow when new files are added to a directory on the server
- automatically create libraries for new users
- replicate the set of tools installed on another server
- automatically assign users to the right group according to email domain

---

## RESTful API

.left[REpresentational State Transfer (REST) is the architectural style of the World Wide Web:]
- client–server
- stateless: no client context stored on the server between requests
- cacheable by client
- Uniform Resource Identifiers (URIs)
- JSON media type
- standard HTTP request methods (GET, POST, PUT, DELETE) and status codes

???

A common way to construct an API command is to view it as a simple (imperative) sentence: `<verb> <a resource>`
- Examples of resources for the Galaxy API are: datasets, tools, jobs, histories, libraries, users... - essentially anything in Galaxy that is recorded in the database.
- The HTTP verbs are GET (i.e. retrieve), POST (create), PUT (update), and DELETE. There are others but they don't apply (yet). These are [HTTP request methods](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods).
  - GET calls should produce no side effects.
  - PUT and DELETE should be idempotent, i.e. should produce the same result no matter how many times they are repeated.
- Often additional options and parameters must be passed to specify how the command should take place.

---

## Resources, URIs, and parameters

- API call: **HTTP method + URI [+ payload]**

  - `GET https://usegalaxy.org/api/histories?order=name` -> ordered list of histories
  - `POST /api/histories {"name": "New"}` -> create a history named "New"
  - `PUT /api/histories/<id> {"published": true}` -> publish a history
  - `DELETE /api/histories/<history_id>/contents/<id>` -> delete a history dataset

- URI parameters: ids in path, others in query (`?name=value&...`)
- POST/PUT payload as JSON

---

## JSON format

.left[JavaScript Object Notation https://www.json.org/] <img style="float: right;" height="80" width="80" alt="JSON logo" src="../../images/json160.gif" />
  - Lightweight data-interchange text format
  - Easy to read/write for both humans and machines
  - ECMA-404 open standard (2013), [RFC 8259](https://tools.ietf.org/html/rfc8259) (2017)

```json
{"history_id": "b5731bb49a17bf50",
 "id": "df06cc665d85b6ea",
 "inputs": {"0": {"id": "bbd44e69cb8906b51528b5d606d1fdd0",
                  "src": "hda"}},
 "model_class": "WorkflowInvocation",
 "outputs": ["bbd44e69cb8906b528819eaaff340ecd",
             "0ff30b4e2a4bed9e"],
 "state": "scheduled",
 "update_time": "2015-07-03T19:28:39.544574",
 "workflow_id": "56482e194d798eb6"}
```

???

Data returned from (and in some cases passed to) the Galaxy API is often described or formatted in JSON.

---

## Status codes and errors

- HTTP status codes:
  - 200 OK, 400 your error, 500 server error, ...<br>
    https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml
- Galaxy error codes and messages: `galaxy/lib/galaxy/exceptions/`
- Still a work in progress :(

---

## How to access a REST API

.left[With anything that can communicate over HTTP:]
- Browsers: only GET
  - All methods by using add-ons, e.g. RESTClient for Firefox, Postman for Chrome
- `wget`: only GET from command line
- `curl`: all methods from command line
- General HTTP programming libraries (e.g. *requests* for Python)
- Service-specific libraries (e.g. *BioBlend* to access Galaxy using Python)

---

## Security

- Most API calls require authentication
  - When the UI accesses the API, session auth is used
  - Other callers needs an **API key**: alphanumeric string (32 chars) identifying a registered user<br>
    Keep it secure, it’s the same as a username+password!
- HTTPS recommended:
  - **https**://localhost?key=foo is safer due to the encryption of the transmitted data

---

## Advanced Galaxy API config

.left[Options in `config/galaxy.yml`:]
- User impersonation by adding `run_as` in the payload
  ```yaml
  # Optional list of email addresses of API users who can make calls on
  # behalf of other users.
  api_allow_run_as: foo@foo.com
  ```

- Bootstrapping Galaxy

  ```yaml
  # Master key that allows many API admin actions to be used without
  # actually having a defined admin user in the database/config.  Only
  # set this if you need to bootstrap Galaxy, you probably do not want
  # to set this on public servers.
  master_api_key: MASTERLOCK
  ```

---

## Galaxy API pros and cons

- Pros:
  - Distributed with Galaxy
  - Well tested
  - Language agnostic
- Cons:
  - Very low-level

---

name: bioblend
## BioBlend

- BioBlend is a **Python library** that wraps the functionality of Galaxy and CloudMan APIs
- Started by Enis Afgan, Nuwan Goonasekera and Clare Sloggett in 2012. Contributions by the Galaxy Team and the community
- Open source (MIT license)
- Available via PyPI and from https://github.com/galaxyproject/bioblend

---

## BioBlend features

- Stable procedural API
- Python 2.7, 3.5- support
- Wraps all main Galaxy API controllers
- Extensive TravisCI testing:
  - on Galaxy release_14.10 and later
  - \>150 unit tests
- Well-documented on https://bioblend.readthedocs.io

---

## BioBlend limitations

- Python-only (but there are also *blend4j* and *blend4php*)
- Its methods just deserialize the JSON response
  - No isolation from changes in the Galaxy API
  - Need to extract the entity id for further processing
- No explicit modeling of Galaxy entities and their relationships
- Complex operations still need many function calls
  - Need for higher-level functionality

---

## BioBlend.objects

- BioBlend.objects is an extra layer which adds an **object-oriented interface** for the Galaxy API
- Started by Simone Leo, Luca Pireddu and Nicola Soranzo at CRS4 in 2013
- Distributed with BioBlend
- Presently limited to datasets, histories, jobs, libraries, tools and workflows

---

## References

- https://docs.galaxyproject.org/
- http://docs.python-requests.org/
- https://bioblend.readthedocs.org/
- C. Sloggett, N. Goonasekera, E. Afgan. BioBlend: automating pipeline analyses within Galaxy and CloudMan. *Bioinformatics* 29(13), 1685-1686, 2013, doi:[10.1093/bioinformatics/btt199](https://doi.org/10.1093/bioinformatics/btt199)
- S. Leo, L. Pireddu, G. Cuccuru, L. Lianas, N. Soranzo, E. Afgan, G. Zanetti. BioBlend.objects: metacomputing with Galaxy. *Bioinformatics* 30 (19), 2816-2817, 2014, [doi:10.1093/bioinformatics/btu386](https://doi.org/10.1093/bioinformatics/btu386)

---

name: handson
## Before we start
1. Get your Galaxy API key:
   1. In your browser, open your Galaxy homepage (e.g. https://usegalaxy.org/ )
   2. Log in or create an account, if you don't have one yet
   3. Go to your `User -> Preferences` in the top menu bar, then click on `Manage API key`
   4. Click on `Create a new key` to generate an API key
   5. Copy your API key to somewhere convenient, you will need it throughout this tutorial

2. Install the tools and workflow on your Galaxy:
	1. The tools:  Click on the Admin tab
		* In the Tools and Tool Shed category, click on the line **Search Tool Shed**
		* Select the **"Galaxy Main Tool Shed"**, and the **"Browse valid repositories"** line
		* Search and install *bam_to_sam* from devteam and *samtools_sort* from IUC owner
	2. The workflow:
		* Get the workflow file (.ga) from
      https://github.com/fmareuil/formationbioblend
		* Import the workflow in galaxy:
		Click on Workflow tab and "Upload or import workflow" button
	3. Launch IPython on a terminal

---

name: familiar
## Connect with Galaxy using IPython:

* Import the GalaxyInstance object from BioBlend module:

  ```python
  from bioblend.galaxy import GalaxyInstance
  ```

* Create your GalaxyInstance instance object using your Galaxy URL and API key

  ```python
  gi = GalaxyInstance(url="http://localhost:8080", key="your key")
  ```

#### Why IPython:
* Automatic completion, e.g. type *gi.* and then press the `Tab` key

* To better understand BioBlend methods and classes, you can use `help(command), object?, object??`

---

name: launch
## Launching a Galaxy job

* Let's look at the help for the **run_tool** method:
  ```python
  help(gi.tools.run_tool)
  ```
* The help should tell you that `run_tool` needs 3 inputs:
  1. A *history_id*, where the input data is and where the output data will be
  2. A *tool_id*, which will tell Galaxy which tool to execute
  3. *tool_inputs*, a dictionary storing the data used to run the tool

---

class: top
## Histories Object

* Get your histories list with BioBlend

  ```python
  list_histories = gi.histories.get_histories()
  ```

--

* Create a new history (*It will be our work history for this tutorial*)

  ```python
  new_history = gi.histories.create_history(name='my_history')
  ```

--

* Now that your history is created, you will need to upload some data to it.

  ```python
  help(gi.histories.upload_dataset_from_library)
  ```

---

class: top
name: library

## Libraries Object

* Check if there is a method to upload a data from your filesystem

  ```python
  help(gi.libraries.upload_file_from_local_path)
  ```

--

* Create a library and set the rights to this library
  - you will need your role *id*, look for the methods of the Class *gi.roles*

  ```python
  role_id = gi.roles.get_roles()[0]['id']
  new_lib = gi.libraries.create_library('my_library')
  gi.libraries.set_library_permissions(new_lib['id'], access_in=[role_id],
        modify_in=[role_id], add_in=[role_id], manage_in=[role_id])
  ```

--

* Import a BAM file in your library

  ```python
  list_data = gi.libraries.upload_file_from_local_path(new_lib['id'], local_path)
  ```

--

* Transfer the BAM file from your library in your new history

  ```python
  data_history = gi.histories.upload_dataset_from_library(new_history['id'],
        list_data[0]['id'])
  ```

---

class: top
name: tool

## Tools Object (1/4)

* To run a tool, its 'id' is needed:

  ```python
  help(gi.tools.run_tool)
  ```

--

* Get the samtools sort tool id

  ```python
  list_tool = gi.tools.get_tools(name='sort')
  ```

---

## Tools Object (2/4)

* The dictionary `tool_inputs` is needed to run a tool

* It is a Python dictionary defined by specific methods from the `bioblend.galaxy.tools.inputs` class

  ```python
  from bioblend.galaxy.tools.inputs import inputs
  ```

* The `inputs` method instantiates a class called `InputsBuilder`:

  ```python
  help(inputs)
  ```

* Each input from the tool XML needs to be defined using the methods `set_param` or `set_dataset_param` from the `InputsBuilder` class
==> If the input format is "data", the method to use is `set_dataset_param`

* Here is an example:

  ```python
  myinputs = inputs().set_param("param1",'value')
        .set_dataset_param("data1",'dataset_id',src="hda")
  ```

**To run the tool samtools sort, we need more information on the tool itself**

---

class: top
## Tools Object (3/4)

* Get the details on the "samtool sort" tool

  ```python
  detail_tool = gi.tools.show_tool(list_tool[0]['id'], io_details=True)
  ```

--

```python
detail_tool['inputs']
[{u'argument': None,
  u'edam_formats': [u'format_2572'],
  u'extensions': [u'bam'],
  ...
  u'multiple': False,
  u'name': u'input1', <---------------------------------------|
  u'optional': False,                                         |
  u'options': {u'hda': [], u'hdca': []},                      |
  u'type': u'data'},                                          | Critical
  {...                                                         | information
  u'name': u'sort_mode', <------------------------------------|
  u'optional': False,                                         |
  u'options': [[u'Chromosomal coordinates', u'', True], <-----|
    [u'Read names', u'-n', False]],
  u'type': u'select',
  u'value': u''}]
```

---

class: top
## Tools Object (4/4)

* Inputs information can be displayed with `detail_tool['inputs']`, use this to instantiate the inputs object

  ```python
  myinputs = inputs().set_dataset_param("input1", data_history['id'], src='hda') \
      .set_param("sort_mode", "")
  ```

--

* Now you can launch `samtool sort`

  ```python
  gi.tools.run_tool(new_history['id'], detail_tool['id'], myinputs)
  ```

---

class: top
name: workflow
## Workflows Object (1/2)

* We are going to use the same process now to launch a workflow
* To know what are the inputs needed:

  ```python
  help(gi.workflows.run_workflow)
  ```

--

* Get the workflows list using the `get_workflows` method from the class `Workflow`
* Get the workflow details using `show_workflow`

  ```python
  my_workflow = gi.workflows.get_workflows(name="wf_formation (imported from uploaded file)")
  detailworkflow = gi.workflows.show_workflow(my_workflow[0]['id'])
  ```

---

class: top
## Workflows Object (2/2)

* Build the `dataset_map` (*key = "input workflow id key" value = {id : 'dataset id', src : 'location of the data'}*)

  ```python
  dataset_map = {}
  dataset_map[detailworkflow['inputs'].keys()[0]] = {'id' : data_history['id'],
                                                  'src' : 'hda'}
  ```

--

* Launch the workflow

  ```python
  gi.workflows.run_workflow(detailworkflow['id'], dataset_map=dataset_map,
                      history_id=new_history['id'])
  ```

**Now you can try to test Bioblend with other tools and workflows.**
